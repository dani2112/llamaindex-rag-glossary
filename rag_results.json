{"question":{"0":"What is the similarity map?","1":"How can I install Renumics Spotlight?","2":"I want to find duplicates in my data. How can I do this?","3":"I want to look at my audio data. Which possibilities does Spotlight offer?"},"ground_truth":{"0":"A similarity map is a visual representation or a data structure that shows how different pieces of data are similar to each other.","1":"You can install Renumics Spotlight by downloading it from the official website and following the installation instructions.","2":"To find duplicates in your data, you can use data deduplication tools or software that scans your dataset for duplicate entries.","3":"Spotlight offers features like audio visualization, audio data analysis, and the ability to search through audio files using metadata or content-based queries."},"answer":{"0":" The Similarity Map is an interactive visualization component in Renumics Spotlight that projects datapoints onto a 2D map based on the similarity of selected columns from datasets loaded using the Hugging Face library. The similarity can be measured by pre-computed descriptions like embeddings or by one or multiple scalar columns in the dataset. Users can select columns to 'placeBy' in the settings, and the dimension reduction arranges the data on the map accordingly using algorithms like UMAP or PCA. The map can be moved, zoomed, and selected points can be added or removed using mouse controls. Coloring and sizing can also be used to represent additional columns on the map. Renumics Spotlight directly works on top of the Hugging Face datasets library, allowing for efficient data visualization and inspection without the need for data copying or pre-processing.","1":" To install Renumics Spotlight, you can use pip by running the command `pip install renumics-spotlight` in your terminal or command prompt. It's recommended to install Spotlight in a separate virtual environment for your project.\n\nYou can use Renumics Spotlight to explore datasets using either the Python API or the command line interface (CLI). Here's how to do it:\n\n**Python API:**\n\nTo use the Python API, first import the necessary libraries and load your dataset using pandas:\n\n```python\nimport pandas as pd\nfrom renumics import spotlight\n\ndf = pd.read_csv(\"path\/to\/your\/dataset.csv\")\nspotlight.show(df)\n```\n\nYou can also specify custom column types for the browser viewer using the `dtype` argument:\n\n```python\nspotlight.show(df, dtype={\"column1\": \"type1\", \"column2\": \"type2\"})\n```\n\n**Command Line Interface (CLI):**\n\nTo use the CLI, load your dataset using `curl` and then run the Spotlight command:\n\n```bash\ncurl \"path\/to\/your\/dataset.csv\" -o dataset.csv\nspotlight dataset.csv --dtype column1=type1 --dtype column2=type2\n```\n\nFor Hugging Face datasets, you can load them directly using the `datasets` library:\n\n```python\nimport datasets\nfrom renumics import spotlight\n\nds = datasets.load_dataset('dataset_name', 'version', split='all')\nspotlight.show(ds)\n```\n\nTo explore an enriched dataset with custom visualization layouts, you can join the raw dataset with the enrichments in a separate dataframe:\n\n```python\nds_enrichments = datasets.load_dataset('enrichments_dataset_name', 'version', split='all')\nds = datasets.concatenate_datasets([ds, ds_enrichments], axis=1)\n```\n\nThen, you can configure the inspector view to choose how each datapoint should be represented using the available visualization and interaction components for different data types such as text, categorical data, image data, and audio data. For more details on configuring the inspector view, please refer to the provided documentation.","2":" To find duplicates in your data, you can use the SliceGuard library to detect problematic clusters in addition to the Annoy library for finding nearest neighbors based on embeddings. Here's how you can do it:\n\n1. Install the required packages: `!pip install renumics-spotlight datasets SliceGuard`\n2. Write a function `cv_issues_cleanvision(df, image_name='image')` that takes your DataFrame `df` and the name of the image column `image_name`, enriches the dataset with metadata using Cleanvision, and returns the enriched DataFrame.\n3. Write a function `find_data_slices(df, categories, category_types={}, spotlight_dtype={}, embedding_name='embedding', label_name='label', prediction_name='prediction')` that takes your DataFrame `df`, the names of the categories you're interested in `categories`, the category types `category_types`, the DataFrame data types for Spotlight `spotlight_dtype`, the name of the embedding column `embedding_name`, the name of the label column `label_name`, and the name of the prediction column `prediction_name`, finds data slices using SliceGuard, and returns the sliced DataFrame.\n4. Load your dataset into a DataFrame `df`.\n5. Enrich the dataset with metadata using Cleanvision: `df_cv = cv_issues_cleanvision(df)`.\n6. Find data slices using SliceGuard: `df_slices = find_data_slices(df_cv, categories, category_types, spotlight_dtype, embedding_name, label_name, prediction_name)`.\n7. Inspect and remove duplicates using Spotlight: `spotlight.show(df_slices, dtype=spotlight_dtype)`.\n\nThis will help you find duplicates based on various data quality issues detected by SliceGuard and similarity of their embeddings using Annoy index.","3":" Based on the context information provided, Spotlight offers two possibilities to view audio data: the `audio()` function and the `spectrogram()` function. The `audio()` function adds an audio viewer to the Spotlight inspector widget, while the `spectrogram()` function adds an audio spectrogram viewer to the Spotlight inspector widget. Both functions support a single column of type `spotlight.Audio` with an optional second column of type `spotlight.Window`."},"contexts":{"0":["---\ntags: []\nid: similarity-map\nsidebar_position: 30\nslug: \/docs\/custom-visualizations\/ui-components\/similarity-map\n---\n\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Similarity Map\n\nThe Similarity Map projects the datapoints onto a 2D Map based on the similarity of the selected columns to place by. The Similarity is measured by either pre computed descriptions (e.G. embeddings retrieved from a ML workflow) or based on one or multiple scalar columns in the dataset.\n\n## Placement\n\nSimply select one ore more columns to \u2018placeBy\u2019 in the settings and watch the dimension reduction arranging the data on the map.\n\nIn order to compute the dimension reduction we offer two different algorithms:\n[UMAP](https:\/\/arxiv.org\/abs\/1802.03426) and [PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis).\nIf the columns values have different distributions,\nThe reduction might run into problems and overestimate the impact of some columns.\nTo counteract this problem, a robust distance metric is available in the advanced settings.\n\nPer default only reduced settings are available to control UMAP\nwhich help you in configuring the reduction to place the samples with more weight on local vs. global similarities.\nHowever, there also are advanced settings available in order give more control over the dimension reduction.\n\nAt the moment the samples can be placed by number columns and Embedding\/Array columns.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/placement\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - place datapoints on the **similarity map** based on a column\"\n\/>\n\n## Controls\n\nThe map can be moved and zoomed by using the mouse.\n\nZooming can be done with the mousewheel, resetting zoom can be done with the `Fit points` button.\n\nThe map can be moved by clicking the `middle mouse` button or with the `left mouse` button if the `alt` key is pressed simultaneously.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/controls\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - navigate in the **similarity map**\"\n\/>\n\n## Selection\n\nSimilar to the [data table](data-table), a single row can be added or removed from the selection.\n\nIn order to select a single point simply click on it. This will reset any selection and exclusively select the clicked point.\n\nTo add a point to a selection click on it while pressing `shift` and to remove it press `ctrl` while clicking on it.\n\nThese steps also apply for multiple points. By pressing the left mouse button and moving the mouse,\na selection rectangle will be shown. On releasing the left mouse button, the selection will be applied in the same fashion as it is done for a single point.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/selection\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - select and deselect points in the **similarity map**\"\n\/>\n\n## Coloring and Sizing\n\nColoring and sizing can be used to represent more columns on the map. The coloring palette can be altered in the global coloring settings.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/coloring-scaling\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - colorize and scale datapoints on the **similarity map** based on a column\"\n\/>\n\n## Filtering\n\nPer default the similarity map also accounts for similarities of filtered out points.\nIn order to compute the similarities only on the filtered points,\n`hide unfiltered` has to be checked.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/filtering\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - show\/hide filtered datapoints on the **similarity map**\"\n\/>","`similaritymap(name=None, columns=None, reduction_method=None, color_by_column=None, size_by_column=None, filter=False, *, umap_metric=None, umap_balance=None, pca_normalization=None)` {similaritymap()}\n\nAdd configured similarity map to Spotlight layout.","---\nsidebar_position: 40\nslug: \/docs\/data-centric-ai\/playbook\n---\n\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\nimport Video from \"@site\/src\/components\/ResponsiveVideo\";\nimport PlaybookChoooser from \"@site\/src\/components\/PlaybookChooser\";\n\n# \ud83c\udfc0 Playbook\n\nThis is a collection of useful data curation workflows (_plays_) on unstructured data.\nWe distinguish between basic data enrichment workflows (_rookie plays_),\nestablished data curation solutions (_veteran plays_) and current state-of-the-art techniques (_all-star plays_).\n\n<PlaybookChoooser \/>\n\n### Rookie plays\n\n-   [Create embeddings with Huggingface](.\/create_huggingface_embedding.mdx)\n-   [Create embeddings with towhee](.\/create_towhee_embedding.mdx)\n-   [Extract decision boundary based on probability ratios](.\/detect_decision_boundary.mdx)\n\n### Veteran plays\n\n-   [Detect duplicates with Annoy](.\/detect_duplicates_annoy.mdx)\n-   [Detect leakage with Annoy](.\/detect_leakage_annoy.mdx)\n-   [Detect data drift](.\/drift_kcore.mdx)\n-   [Detect label errors](.\/label_errors_cleanlab.mdx)\n-   [Detect outliers](.\/outlier_cleanlab.mdx)\n-   [Detect image error patterns](.\/cv_issues.mdx)\n\n### All-star plays\n\n-   [Find data slices](.\/find_data_slices_sliceguard.mdx)","Leveraging model results such as predictions or embeddings can help to uncover critical data samples and clusters. In practice, these enrichments can be stored in a separate dataframe and then joined with the raw dataset. Here is an example from the Huggingface hub:\n\n```python\nimport datasets\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\nds_results = datasets.load_dataset('renumics\/speech_commands-ast-finetuned-results', 'v0.01', split='all')\nds = datasets.concatenate_datasets([ds, ds_results], axis=1)\n```\n\nDepending on the task at hand (e.g. EDA, model debugging, monitoring), you probably want to set up a suitable visualization layout. You can do so in the GUI and via API. We also ship starter layouts for common tasks that you can use out of the box:\n\n```python\nfrom renumics import spotlight\n\nlayout = spotlight.layouts.debug_classification(embedding='embedding', inspect={'audio': spotlight.dtypes.audio_dtype})\nspotlight.show(ds, dtype={'embedding': spotlight.Embedding}, layout=layout )\n```\n\n## Disclaimer\n\n<details>\n    <summary>Usage Tracking<\/summary>\n\nWe have added crash report and perfomance collection.<br \/>\n**We do NOT** collect user data other than an **anonymized Machine Id** obtained by py-machineid, and only log our own actions.<br \/>\n**We do NOT** collect folder names, dataset names, or row data of any kind only aggregate performance statistics like total time of a table_load, crash data, etc.<br \/>\nCollecting spotlight crashes will help us improve stability.<br \/>\n\n<br \/>\n<br \/>\nToo opt out of the crash report collection define an environment variable called\nSPOTLIGHT_OPT_OUT and set it to true.\n\ne.G.\n\n```bash\nexport SPOTLIGHT_OPT_OUT=true\n```\n\n<\/details>","---\ntitle: \ud83e\udd17 Loading Huggingface Data\nslug: \/docs\/loading-data\/huggingface\nsidebar_position: 30\n---\n\n# Loading Data from Huggingface datasets\n\n![Huggingface example](\/img\/speech_commands_vis_s.gif)\n\nThe Hugging Face datasets library not only provides access to more than 70k publicly available datasets, but also offers very convenient data preparation pipelines for custom datasets.\n\nRenumics Spotlight allows you to create interactive visualizations to identify critical clusters in your data. Because Spotlight understands the data semantics within Hugging Face datasets, you can get started with just one line of code:\n\n```python\nimport datasets\nfrom renumics import spotlight\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='validation')\n\nspotlight.show(ds)\n```\n\n## Spotlight \ud83e\udd1d Hugging Face datasets\n\nThe datasets library has several features that makes it an ideal tool for working with ML datasets: It stores tabular data (e.g. metadata, labels) along with unstructured data (e.g. images, audio) in a common Arrows table. Datasets also describes important data semantics through features (e.g. images, audio) and additional task-specific metadata.\n\nSpotlight directly works on top of the datasets library. This means that there is no need to copy or pre-process the dataset for data visualization and inspection. Spotlight loads the tabular data into memory to allow for efficient, client-side data analytics. Memory-intensive unstructured data samples (e.g. audio, images, video) are loaded lazily on demand. In most cases, data types and label mappings are inferred directly from the dataset. Here, we visualize the CIFAR-100 dataset with one line of code:\n\n```python\nds = datasets.load_dataset('cifar100', split='test')\nspotlight.show(ds)\n```\n\nIn cases where the data types are ambiguous or not specified, the Spotlight API allows to manually assign them:\n\n```python\nlabel_mapping = dict(zip(ds.features['fine_label'].names, range(len(ds.features['fine_label'].names))))\nspotlight.show(ds, dtype={'img': spotlight.Image, 'fine_label': spotlight.dtypes.CategoryDType(categories=label_mapping)})\n```\n\n## Supported data types\n\nMore details on the supported data types can be found in the [data types section](..\/loading-data#supported-data-types)\n\n## Detailed examples\n\nTake a look at our [use case section](..\/use-cases) to find more detailed examples for different modalities."],"1":["---\nslug: \/docs\/development\ntitle: Contributing to Spotlight\n---\n\n## Setting up Spotlight for development\n\nIn order to start development in spotlight as of now the easiest method is\nto checkout the repository and install development dependencies.\n\nThe spotlight backend is a [FastAPI](https:\/\/fastapi.tiangolo.com\/em\/) server written in `python`, serving the dataset\nand the frontend.<br \/>\nThe frontend is a [React](https:\/\/reactjs.org\/) application written in `typescript`.\n\nTherefore, for development, you'll need to install both `python` together with `poetry`\nand `nodejs` with `pnpm` to get started.\n\n:::info\nIf you want to contribute to spotlight in any form and struggle anywhere along the way please reach out to us.\n\nWe are more than happy to help out and guide you through the process.\n:::\n\n### Install dependencies\n\n#### Developing on Linux\n\nInstall [python3](https:\/\/www.python.org\/) together with [poetry](https:\/\/python-poetry.org\/docs\/)\n\n```bash\nsudo apt update\nsudo apt install python3 python3-dev\ncurl -sSL https:\/\/install.python-poetry.org | python3 -\n```\n\nInstall [nodejs](https:\/\/nodejs.org\/en\/) together with [pnpm](https:\/\/pnpm.io\/)\n\n```bash\nsudo apt install nodejs\ncurl -fsSL https:\/\/get.pnpm.io\/install.sh | sh -\n\n# check your installed node version\nnode -v\n```\n\nWe recommend using at least node version 18.\nIn order to install the latest version check out [NodeSource on Github](https:\/\/github.com\/nodesource\/distributions)\n\nYou might need to restart your terminal in order to use `pnpm` as a command.\n\n### Setup Spotlight Repository\n\nVisit the [spotlight repository](https:\/\/github.com\/Renumics\/spotlight) and click **Fork**.\nSetup your forked repository by cloning it and adding spotlight as an additional remote.\n\n```bash\ngit clone https:\/\/github.com\/YOUR_GIT_USERNAME\/spotlight.git\ncd spotlight\ngit remote add upstream https:\/\/github.com\/renumics\/spotlight.git\n```\n\nInside the spotlight repository you'll find a `Makefile` which contains all the commands to get you started.\nFirst install development dependencies and pre-commit.\n\n```bash\nmake init\n```\n\nand run the development server\n\n```bash\nmake dev\n```\n\n## Submit your work\n\nPlease make sure that you have [pre-commit](https:\/\/pre-commit.com\/) hooks installed and\nthat the hooks successfully ran for the added changes.<br \/>\nThis can be verified by creating a commit and checking if automated tests are run before the commit is created.\n\n```bash\npoetry run pre-commit install --hook-type pre-commit\npoetry run pre-commit install --hook-type pre-push\n```\n\nSubmit your improvements, fixes and new features to Spotlight by creating a\n[Pull Request](https:\/\/opensource.guide\/how-to-contribute\/#opening-a-pull-request).\n\n## Using [direnv](https:\/\/direnv.net\/)\n\nIn order to make development easier [direnv](https:\/\/direnv.net\/) can be used to automatically setup the environment\non entering the spotlight folder.\n\nThe provided [.envrc](https:\/\/github.com\/Renumics\/spotlight\/blob\/main\/.envrc) file automatically activates\nthe poetry environment and sets environment variables in .env and .env.local.","`inspector(name=None, lenses=None, num_columns=4)` {inspector()}\n\nAdd an inspector widget with optionally preconfigured viewers (lenses).\n\n Example\n\n```python\n>>> from renumics.spotlight import layout\n>>> from renumics.spotlight.layout import lenses\n>>> spotlight_layout = layout.layout(\n...     layout.inspector(\n...         \"My Inspector\",\n...         [\n...             lenses.scalar(\"bool\"),\n...             lenses.scalar(\"float\"),\n...             lenses.scalar(\"str\"),\n...             lenses.scalar(\"datetime\"),\n...             lenses.scalar(\"category\"),\n...             lenses.scalar(\"int\"),\n...             lenses.text(\"str\", name=\"text\"),\n...             lenses.html(\"str\", name=\"HTML (safe)\"),\n...             lenses.html(\"str\", name=\"HTML\", unsafe=True),\n...             lenses.markdown(\"str\", name=\"MD\"),\n...             lenses.array(\"embedding\"),\n...             lenses.array(\"window\"),\n...             lenses.array(\"array\"),\n...             lenses.sequences(\"sequence\"),\n...             lenses.sequences([\"sequence1\", \"sequence2\"], name=\"sequences\"),\n...             lenses.mesh(\"mesh\"),\n...             lenses.image(\"image\"),\n...             lenses.video(\"video\"),\n...             lenses.audio(\"audio\"),\n...             lenses.audio(\"audio\", window_column=\"window\", name=\"windowed audio\"),\n...             lenses.spectrogram(\"audio\"),\n...             lenses.spectrogram(\n...                 \"audio\",\n...                 window_column=\"window\",\n...                 name=\"windowed spectrogram\",\n...             ),\n...         ],\n...         num_columns=2,\n...     )\n... )\n```","---\nslug: \/docs\/getting-started\nsidebar_position: 10\n---\n\nimport ReactPlayer from \"react-player\";\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\n\n# \ud83d\ude80 Getting Started\n\n> Spotlight helps you to **understand unstructured datasets** fast. You can create **interactive visualizations** from your dataframe with just a few lines of code. You can also leverage data enrichments (e.g. embeddings, prediction, uncertainties) to **identify critical clusters** in your data.\n\n![Dataframe example](\/img\/speech_commands_vis_s.gif)\n\n## What you'll need\n\n-   [Python](https:\/\/www.python.org\/downloads\/) version 3.8-3.11\n\n## Install Spotlight via [pip](https:\/\/packaging.python.org\/en\/latest\/key_projects\/#pip)\n\n```bash\npip install renumics-spotlight\n```\n\n> We recommend installing Spotlight and everything you need to work on your data in a separate [virtual environment](https:\/\/docs.python.org\/3\/tutorial\/venv.html)\n\n## Load your first dataset\n\nData can be loaded into Spotlight either from memory (as Python object) or from file. We currently support Pandas dataframes as well as Huggingface datasets and our own HDF5-based dataset format. Depending on the format, unstructured data samples are either stored directly in the dataframe or as a path to a separate file.\n\nThis is an example how your dataframe might look like:\n\n![Dataframe example](\/img\/dataframe_head_sample.png)\n\nYou can directly load your dataframe either via the Python API or the command line interface (CLI):\n\n<Tabs groupId=\"interface\">\n<TabItem value=\"python\" label=\"python\" default>\n\n```python\nimport pandas as pd\nfrom renumics import spotlight\n\ndf = pd.read_csv(\"https:\/\/renumics.com\/data\/mnist\/mnist-tiny.csv\")\nspotlight.show(df, dtype={\"image\": spotlight.Image, \"embedding\": spotlight.Embedding})\n```\n\n-   `pd.read_csv` loads a sample csv file as a pandas [DataFrame](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html).\n-   `spotlight.show` opens up spotlight in the browser with the pandas dataframe ready for you to explore.\n-   The `dtype` argument specifies custom column types for the browser viewer.\n\n<\/TabItem>\n<TabItem value=\"bash\" label=\"CLI\">\n\n```bash\ncurl https:\/\/renumics.com\/data\/mnist\/mnist-tiny.csv -o mnist-tiny.csv\nspotlight mnist-tiny.csv --dtype image=Image --dtype embedding=Embedding\n```\n\n<\/TabItem>\n<\/Tabs>\n\n## Load a [Hugging Face](https:\/\/huggingface.co\/) dataset\n\nHuggingface datasets have a rich semantic description of the feature columns. Spotlight can thus parse data type descriptions and label mappings automatically. This means that creating a visualization is typically as simple as:\n\n```python\nimport datasets\nfrom renumics import spotlight\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\n\nspotlight.show(ds)\n```\n\n## Exploring an enriched dataset with custom visualization layouts\n\nExploring raw unstructured datasets often yield little insights. Leveraging model results such as predictions or embeddings can help to uncover critical data samples and clusters. In practice, these enrichments can be stored in a separate dataframe and then joined with the raw dataset. Here is an example from the Huggingface hub:\n\n```python\nimport datasets\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\nds_results = datasets.load_dataset('renumics\/speech_commands-ast-finetuned-results', 'v0.01', split='all')\nds = datasets.concatenate_datasets([ds, ds_results], axis=1)\n```\n\nDepending on the task at hand (e.g. EDA, model debugging, monitoring), you probably want to set up a suitable visualization layout. You can do so in the GUI and via API.","`mesh(column, name=None, *, color_attribute=None, wireframe=False, transparency=0, animation='loop', animation_scale=1.0, synchronize=True)` {mesh()}\n\nAdd mesh viewer to Spotlight inspector widget.\n\nSupports a single column of type `spotlight.Mesh`.","---\ntags: []\nid: inspector\nsidebar_position: 20\nslug: \/docs\/custom-visualizations\/ui-components\/inspector\n---\n\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Inspector\n\nThe Inspector Widget is a component in Spotlight that lets you examine and edit the features of **individual data points** in your dataset.\nWith its multiple views for **different modalities**, the Inspector Widget provides a detailed look at a data point's underlying structure.\n\nThe Inspector Widget's views for text, audio, video, and 3D geometry data allow you to explore a data point's features in depth.\nYou can also edit those features directly within the view, making it easy to make changes to a data point without navigating to a separate screen.\nThe Inspector Widget is an essential tool for working with multimodal datasets and gives you greater control over the structure and quality of your data.\n\n:::info\n\nThe open source version of Renumics Spotlight allows you to inspect your data to find critical segments. The Pro version of Spotlight also enables data tagging and annotation.\n\n:::\n\nBy configuring the inspector view you can choose how each datapoint should be represented. The main configuration options are available in the upper right corner of the view:\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/inspector-view-overview\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720, 1280]}\n    annotation=\"_fsd50k_ - add and layout components in the **Inspector View**\"\n\/>\n\nThere are visualization and interaction components available for many different data types. These include:\n\n### Text data\n\n**Text data** can be represented either with a **Value** view or a **Select** view that features autocomplete.\n\n<!-- poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\/text-value-field.jpg\" -->\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field-512.jpg\"\n    annotation=\"_fsd50k_ - display annotation in a **Value** view\"\n\/>\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field.jpg\"\n    annotation=\"_fsd50k_ - display and update annotation in a **Select** view\"\n\/>\n\n### Categorical data\n\n**Categorical data** is represented similarly to **Text data** with the exception, that when there are not too many categories, they can also be edited with a switch component.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field-512.jpg\"\n    annotation=\"_mnist_ - display and update categories in a **Switch** view\"\n\/>\n\n### Image data\n\n**Image data** can be represented with an imager viewer that supports zooming and panning.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/image-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/image-view-512.jpg\"\n    annotation=\"_mnist_ - display and zoom\/pan images in a **Image** view\"\n\/>\n\n### Audio data\n\n**Audio data** can be represented with an _Audio Player_ or a _Spectrogram_. The _Audio Player_ supports annotating event windows in the data."],"2":["---\ntags: []\nid: duplicates-annoy\nsidebar_position: 35\nslug: \/docs\/data-centric-ai\/playbook\/duplicates-annoy\n---\n\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Detect duplicates with Annoy\n\nWe use embeddings to detect duplicates by computing nearest neighbors with the Annoy library. Although the example is based on image embeddings, the basic play is independent of the data type.\n\n> Use Chrome to run Spotlight in Colab. Due to Colab restrictions (e.g. no websocket support), the performance is limited. Run the notebook locally for the full Spotlight experience.\n\n<a\n    target=\"_blank\"\n    href=\"https:\/\/colab.research.google.com\/github\/Renumics\/spotlight\/blob\/main\/playbook\/veteran\/duplicates_annoy.ipynb\"\n>\n    <img\n        src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\"\n        alt=\"Open In Colab\"\n    \/>\n<\/a>\n\n<Tabs groupId=\"usedDatasets\" >\n<TabItem value=\"inputs\" label=\"inputs\">\n\n-   `df['embedding']` contain the [embeddings](..\/glossary\/index.mdx#embedding) for each data sample\n\n<\/TabItem>\n<TabItem value=\"outputs\" label=\"outputs\">\n\n-   `df['nn_id']` contains the sample id for the [nearest neighbor](..\/glossary\/index.mdx#nearest-neighbor) in the embedding space.\n-   `df['nn_image']` contains the path to the [image](..\/glossary\/index.mdx#image-data) that is the nearest neighbor.\n-   `df['nn_distance']` contains distance to the nearest neighbor.\n-   `df['nn_flag']` contains a flag that indicates if the sample is a near-duplicate according to the given threshold.\n\n<\/TabItem>\n<TabItem value=\"parameters\" label=\"parameters\">\n* `threshold` denotes the distance threshold when a data sample is considered a near-duplicate.\n* `tree_size` is an internal parameter for the Annoy library that calibrates the speed-efficiency tradeoff. More trees gives higher precision when querying.\n<\/TabItem>\n<\/Tabs>\n\n![Spotlight_screenshot_decision_boundary](\/playbook\/duplicates_annoy_screenshot.png)\n\n## Imports and play as copy-n-paste functions\n\n<details>\n    <summary># Install dependencies<\/summary>\n\n```python\n#@title Install required packages with PIP\n\n!pip install renumics-spotlight datasets annoy\n```\n\n<\/details>\n\n<details>\n    <summary># Play as copy-n-paste functions<\/summary>\n\n```python\n#@title Play as copy-n-paste functions\n\nimport datasets\nfrom renumics import spotlight\nfrom annoy import AnnoyIndex\nimport pandas as pd\nimport requests\n\n\ndef nearest_neighbor_annoy(df, embedding_name='embedding', threshold=0.3, tree_size=100):\n\n    embs = df[embedding_name]\n\n    t = AnnoyIndex(len(embs[0]), 'angular')\n\n    for idx, x in enumerate(embs):\n          t.add_item(idx, x)\n\n    t.build(tree_size)\n\n    images = df['image']\n\n    df_nn = pd.DataFrame()\n\n    nn_id = [t.get_nns_by_item(i,2)[1] for i in range(len(embs))]\n    df_nn['nn_id'] = nn_id\n    df_nn['nn_image'] = [images[i] for i in nn_id]\n    df_nn['nn_distance'] = [t.get_distance(i, nn_id[i]) for i in range(len(embs))]\n    df_nn['nn_flag'] = (df_nn.nn_distance < threshold)\n\n\n    return df_nn\n```\n\n<\/details>\n\n## Step-by-step example on CIFAR-100\n\n### Load CIFAR-100 from Huggingface hub and convert it to Pandas dataframe\n\n```python\ndataset = datasets.load_dataset(\"renumics\/cifar100-enriched\", split=\"train\")\ndf = dataset.to_pandas()\n```\n\n### Compute nearest neighbors including distances\n\n```python\ndf_nn = nearest_neighbor_annoy(df)\ndf = pd.concat([df, df_nn], axis=1)\n```\n\n### Inspect and remove duplicates with Spotlight\n\n```python\ndf_show = df.drop(columns=['embedding', 'probabilities'])\nlayout_url = \"https:\/\/raw.githubusercontent.com\/Renumics\/spotlight\/playbook_initial_draft\/playbook\/rookie\/duplicates_annoy.json\"\nresponse = requests.get(layout_url)\nlayout = spotlight.layout.nodes.Layout(**json.loads(response.text))\nspotlight.show(df_show, dtype={\"image\": spotlight.Image, \"embedding_reduced\": spotlight.Embedding}, layout=layout)\n```","`check_column_name(name)` {Dataset.check_column_name()}\n\nCheck a column name.\n\nInstance variables","Leveraging model results such as predictions or embeddings can help to uncover critical data samples and clusters. In practice, these enrichments can be stored in a separate dataframe and then joined with the raw dataset. Here is an example from the Huggingface hub:\n\n```python\nimport datasets\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\nds_results = datasets.load_dataset('renumics\/speech_commands-ast-finetuned-results', 'v0.01', split='all')\nds = datasets.concatenate_datasets([ds, ds_results], axis=1)\n```\n\nDepending on the task at hand (e.g. EDA, model debugging, monitoring), you probably want to set up a suitable visualization layout. You can do so in the GUI and via API. We also ship starter layouts for common tasks that you can use out of the box:\n\n```python\nfrom renumics import spotlight\n\nlayout = spotlight.layouts.debug_classification(embedding='embedding', inspect={'audio': spotlight.dtypes.audio_dtype})\nspotlight.show(ds, dtype={'embedding': spotlight.Embedding}, layout=layout )\n```\n\n## Disclaimer\n\n<details>\n    <summary>Usage Tracking<\/summary>\n\nWe have added crash report and perfomance collection.<br \/>\n**We do NOT** collect user data other than an **anonymized Machine Id** obtained by py-machineid, and only log our own actions.<br \/>\n**We do NOT** collect folder names, dataset names, or row data of any kind only aggregate performance statistics like total time of a table_load, crash data, etc.<br \/>\nCollecting spotlight crashes will help us improve stability.<br \/>\n\n<br \/>\n<br \/>\nToo opt out of the crash report collection define an environment variable called\nSPOTLIGHT_OPT_OUT and set it to true.\n\ne.G.\n\n```bash\nexport SPOTLIGHT_OPT_OUT=true\n```\n\n<\/details>","---\ntags: []\nid: text-classification\nslug: \/docs\/use-cases\/text-classification\n---\n\n# Detect Problems in Text Classification Datasets\n\nUse the [**sliceguard**](https:\/\/github.com\/Renumics\/sliceguard) library and [**Spotlight**](https:\/\/github.com\/Renumics\/spotlight) to quickly **detect problem clusters** that can cause issues when training your text classification model. Shows problems such as:\n\n1. Label Inconsistencies\n2. Outliers and Errors\n3. Text-specific Issues (e.g. typos, empty samples)\n\nGenerally it will show you everything that is hard to learn for a classification model. All you need is a dataframe containing **texts** and **labels**.\n\nFirst install the dependencies:\n\n```\npip install renumics-spotlight sliceguard[all] scikit-learn\n```\n\nThen run the following code to detect problematic clusters:\n\n```python\n# The Imports\nfrom renumics import spotlight\nfrom sliceguard import SliceGuard\nfrom sliceguard.data import from_huggingface\nfrom sklearn.metrics import accuracy_score\n\n# Load an Example Dataset as DataFrame\ndf = from_huggingface(\"dair-ai\/emotion\")\n\n# DataFrame Format:\n# +-------+-------+\n# | text  | label |\n# +-------+-------+\n# | text1 | joy   |\n# | text2 | anger |\n# | text3 | joy   |\n# | ...   |       |\n# +-------+-------+\n\n# Detect Issues Using sliceguard\nsg = SliceGuard()\nissues = sg.find_issues(df, features=[\"text\"], y=\"label\", metric=accuracy_score)\nreport_df, spotlight_data_issues, spotlight_dtypes, spotlight_layout = sg.report(\n    no_browser=True\n)\n\n# Visualize Detected Issues in Spotlight:\nspotlight.show(\n    report_df,\n    dtype=spotlight_dtypes,\n    issues=spotlight_data_issues,\n    layout=spotlight_layout,\n)\n```","def find_data_slices(df, categories, category_types={}, spotlight_dtype={}, embedding_name='embedding', label_name='label', prediction_name='prediction'):\n    sg = SliceGuard()\n    df_slices = sg.find_issues(\n        df,\n        categories,\n        label_name,\n        prediction_name,\n        accuracy_score,\n        precomputed_embeddings = {'embedding': df[embedding_name].to_numpy()},\n        metric_mode=\"max\",\n        feature_types=category_types\n    )\n\n    sg.report(spotlight_dtype=spotlight_dtype)\n\n    return df_slices\n```\n\n<\/details>\n\n## Step-by-step example on CIFAR-100\n\n### Load CIFAR-100 from Huggingface hub and convert it to Pandas dataframe\n\n```python\ndataset = datasets.load_dataset(\"renumics\/cifar100-enriched\", split=\"test\")\ndf = dataset.to_pandas()\n```\n\n### Enrich dataset with metadata using [Cleanvision](.\/cv_issues.mdx)\n\n```python\ndef cv_issues_cleanvision(df, image_name='image'):\n\n    image_paths = df['image'].to_list()\n    imagelab = Imagelab(filepaths=image_paths)\n    imagelab.find_issues()\n\n    df_cv=imagelab.issues.reset_index()\n\n    return df_cv\n\ndf_cv = cv_issues_cleanvision(df)\ndf = pd.concat([df, df_cv], axis=1)\n```\n\n### Identify and explore data slices with Sliceguard\n\n```python\ncategories=['dark_score', 'low_information_score', 'light_score', 'blurry_score', 'fine_label']\nprediction = 'fine_label_prediction'\nlabel = 'fine_label'\ncategory_types={'fine_label': 'nominal'}\nspotlight_dtype={\"image\": Image}\n\nfind_data_slices(df, categories, category_types=category_types, spotlight_dtype=spotlight_dtype, embedding_name='embedding', label_name=label, prediction_name=prediction)\n\n```"],"3":["`audio(column, window_column=None, name=None, *, repeat=False, autoplay=False)` {audio()}\n\nAdd audio viewer to Spotlight inspector widget.\n\nSupports a single column of type `spotlight.Audio` with optional second\ncolumn of type `spotlight.Window`.","`spectrogram(column, window_column=None, name=None, *, frequency_scale='linear', amplitude_scale='decibel')` {spectrogram()}\n\nAdd audio spectrogram viewer to Spotlight inspector widget.\n\nSupports a single column of type `spotlight.Audio` with optional second\ncolumn of type `spotlight.Window`.","`viewers()` {viewers()}\n\nGet all active Spotlight viewer instances.","`Viewer(host='127.0.0.1', port='auto')` {Viewer}\n\nA Spotlight viewer. It corresponds to a single running Spotlight instance.\n\nViewer can be created using `spotlight.show`.\n\n Attributes\n\n**`host`**\n: host at which Spotlight is running\n\n**`port`**\n: port at which Spotlight is running\n\nInstance variables","---\nsidebar_position: 30\nslug: \/docs\/configure-visualizations\n---\n\n# Configuring visualizations\n\nData inspection is critical for all steps in the ML development workflow. But it is also time consuming: First of all it takes effort to build and maintain custom visualization pipelines. Second of all, it takes time to sift through data manually.\n\nSpotlight enables you to build scalable data inspection workflows. With Spotlight you can\n\n-   Create visualizations for unstructured data with a single line of code\n-   Leverage model results to find relevant data segments quickly\n-   Create custom interaction templates for your team\n\nIn order to achieve this, the Spotlight UI provides a range of components that offer different perspectives on your data. You can add, remove, and rearrange these components as you wish to create a customized inspection layouts.\n\nOnce you have identified a component layout that works well for you, you can save it as a layout for future use. This way, you can quickly load your preferred layout whenever you need it, without having to reconfigure the UI each time.\n\n## UI components\n\nSpotlight includes the [Inspector](ui_components\/inspector_view.mdx) and the [Data Table](ui_components\/table_view.mdx) to help you in analyzing and understanding individual datapoints. Other visualization options like the [Similarity Map](ui_components\/similarity_map.mdx), the [Scatter Plot](ui_components\/scatter_plot.mdx) and the [Histogram](ui_components\/histogram.mdx) aide you to understand the overall distribution of your data.\n\nThe [Filter Bar](ui_components\/filter_bar.mdx) is a special component that allows you to filter your data based on the values of your features.<br\/>\nFiltering and selecting datapoints can affect how data is presented in the other components and therefore can greatly help you in analyzing and reasoning about your data.\n\nAn overview over all available components can be found in the [UI components](ui_components\/index.mdx) section.\n\n## Layout API\n\n## Issues widget"]}}