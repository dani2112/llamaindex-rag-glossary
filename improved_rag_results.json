{"question":{"0":"What is the similarity map?","1":"How can I install Renumics Spotlight?","2":"I want to find duplicates in my data. How can I do this?","3":"Which possibilities regarding cluster analysis does Spotlight offer and how can it help me?","4":"I want to identify label issues for my audio use case? Which features and visualizations can help me and how could the process look?","5":"What are the main UI elements that help me with my use case in Spotlight?","6":"What is the Inspector and how does it play together with other components?","7":"What is sliceguard and how does it play together with Spotlight?","8":"I want to look at my audio data. Which possibilities does Spotlight offer?","9":"How can I customize the data visualization in Renumics Spotlight?","10":"Which possibilities for data loading and export does Renumics Spotlight offer?"},"ground_truth":{"0":"The similarity map is a dimensionality reduction plot that displays high dimensional data like embeddings in 2D space. It can be very useful for getting a first overview on dataset structure and possible problems, before diving into more detailed analysis.","1":"Just run pip install renumics-spotlight. After that, start it by running spotlight from the CLI or use the Python interface via spotlight.show.","2":"You can either just do that exploratively, e.g. by placing data on the similarity map and checking if nearby datapoints are identical via the inspector. You can also detect duplicates with specialized tooling like annoy and display the results in spotlight via spotlight.show.","3":"Spotlight offers the similarity map, which can display embeddings or feature arrays in 2D. You can then identify clusters visually and browse through them while looking at the underlying data such as images or audio files in the inspector.","4":"Similarity Map, Inspector and Filtering can help you with that. You could simply place your data points on the similarity map using an general purpose audio embedding. You could then color the data points by label. If there are clusters with mixed labels, that's a sign for label issues. You could also train a model on the existing labels and then color by model error, which is a great way of pinpointing the issues even faster.","5":"The main elements are the table for exploring data points by their metadata, the inspector for displaying unstructured data, the filter bar for filtering the data, The similarity map as an overview plot on the dataset structure, and potentially multiple additional visualizations such as histograms or scatterplots, as well as metric display elements such as a confusion matrix.","6":"It is Spotlight's UI component that is able to display unstructured data such as images, audio files, text, and more. It can be used to easily explore unstructured, multimodal datasets.","7":"It is a library that can automatically detect data clusters or slices where a machine learning model does not perform well. It uses Spoglight to display its detection results. It is also built by Renumics.","8":"You can display audio data in the inspector. Spotlight offers the visualizations audio player and spetrogram. It can also display windows in which a certain event in the audio data occurs. To visualize audio dataset structure you could use audio embeddings that are displayed on the Similarity Map.","9":"You can configure the UI flexibly via the UI. This involves configuring a layout with different widgets and plots, as well as adding lenses to the inspector widget. You can also save and load configured layouts either via json files or via the python API.","10":"Spotlight can load pandas dataframes and huggingface datasets via spotlight.show. It can also load hdf5 files (legacy) and csv files. The preferred way is directly loading pandas dataframes via spotlight.show."},"answer":{"0":" The similarity map is a data visualization component that projects datapoints onto a 2D map based on the similarity of selected columns. The similarity can be measured by pre-computed descriptions like embeddings or by one or multiple scalar columns in the dataset. The map can be moved and zoomed using the mouse, and datapoints can be selected and deselected by clicking on them while pressing certain keys. The map can also be colored and sized based on additional columns, and it can be filtered to show only selected points. The reduction algorithms used for placing the samples on the map include UMAP and PCA, with the option to use a robust distance metric for handling columns with different distributions.","1":" To install Renumics Spotlight, you can use pip by running the following command in your terminal or command prompt:\n```\npip install renumics-spotlight\n```\nIt's recommended to install Spotlight in a separate virtual environment for your Python project. For more details on how to use a virtual environment, you can refer to the Python documentation: <https:\/\/docs.python.org\/3\/tutorial\/venv.html>","2":" To find duplicates in your data, you can use the Annoy library along with embeddings to detect nearest neighbors. The example in the provided context uses image embeddings but the basic play is independent of the data type. Here's how you can do it:\n\n1. Install the required packages: `pip install renumics-spotlight datasets annoy`\n2. Write a function `nearest_neighbor_annoy` that takes your DataFrame, the name of the embedding column, threshold, and tree_size as arguments. This function will compute nearest neighbors with the Annoy library and add the nearest neighbor information to the DataFrame.\n3. Load your data into a DataFrame and call the `nearest_neighbor_annoy` function to find nearest neighbors.\n4. Inspect the results using Spotlight to identify duplicates based on the threshold you set.\n\nHere's the code snippet for the `nearest_neighbor_annoy` function:\n\n```python\nimport datasets\nfrom renumics import spotlight\nfrom annoy import AnnoyIndex\nimport pandas as pd\nimport numpy as np\n\ndef nearest_neighbor_annoy(df, embedding_name='embedding', threshold=0.3, tree_size=100):\n    embs = df[embedding_name].to_numpy()\n    index = AnnoyIndex(len(embs), 'euclidean')\n    index.build(embs, tree_size)\n    nn_ids = index.search(np.array(embs), k=1, include_distances=True)\n    df['nn_id'] = np.repeat(nn_ids[:,0], len(embs))\n    df['nn_distance'] = np.repeat(nn_ids[:,1].squeeze(), len(embs))\n    df['nn_flag'] = df['nn_distance'].apply(lambda x: 1 if x < threshold else 0)\n    return df\n```\n\nNow you can use this function to find duplicates in your DataFrame:\n\n```python\ndf = pd.read_csv('your_data.csv') # replace with your data source\ndf_duplicates = nearest_neighbor_annoy(df)\n```\n\nThe `nn_flag` column in the resulting DataFrame will indicate if each sample is a near-duplicate based on the threshold you set.","3":" Spotlight offers various possibilities for cluster analysis through its UI components. It includes the Inspector, Data Table, Similarity Map, Scatter Plot, Histogram, and Filter Bar. These components aid in understanding individual datapoints, overall data distribution, and filtering data based on feature values. By defining specific data clusters through their row and\/or column ids, you can quickly inspect critical issues in your data.","4":" To identify label issues for your audio use case, you can utilize the `sliceguard` library and `Spotlight` for detecting problem clusters in your dataset. This process involves the following steps:\n\n1. First, ensure you have the required dependencies installed:\n   ```\n   pip install renumics-spotlight sliceguard[all] scikit-learn\n   ```\n2. Load your dataset into a DataFrame containing paths to audio files and their corresponding labels.\n3. Use `sliceguard` to detect issues:\n   ```python\n   from renumics import spotlight\n   from sliceguard import SliceGuard\n   from sliceguard.data import from_huggingface\n   from sklearn.metrics import accuracy_score\n\n   df = from_huggingface(\"renumics\/emodb\")  # Load an example dataset\n   issues = sg.find_issues(df, features=[\"audio\"], y=\"emotion\", metric=accuracy_score)\n   ```\n4. Visualize detected issues using Spotlight:\n   ```python\n   report_df, spotlight_data_issues, spotlight_dtypes, spotlight_layout = sg.report(no_browser=True)\n   spotlight.show(report_df, dtype=spotlight_dtypes, issues=spotlight_data_issues, layout=spotlight_layout)\n   ```\n\nFeatures:\n- The dataset should contain paths to audio files and their corresponding labels.\n\nVisualizations:\n- Use `sliceguard` to detect problem clusters in your dataset, which can include label inconsistencies, outliers, errors, and audio-specific issues like clipping. The library provides a report with detected issues that can be visualized using Spotlight.\n- For exploring individual audio files, you can use an _Audio Player_ or _Spectrogram_ visualization to represent audio data. These visualizations can help you understand the audio data better and annotate event windows in the data.","5":" The main UI elements that can help you with your use case in Spotlight for data inspection are the Inspector and the Data Table. The Inspector allows you to examine and edit the features of individual data points in your dataset with multiple views for different modalities such as text, audio, video, and 3D geometry data. The Data Table provides an overview of your data in tabular form, allowing you to filter and sort the data based on the values of your features. Additionally, other visualization options like the Similarity Map, Scatter Plot, and Histogram can aid you in understanding the overall distribution of your data. The Filter Bar is also a special component that allows you to filter your data based on the values of your features, which can greatly help you in analyzing and reasoning about your data.","6":" The Inspector is a widget in Spotlight, a data exploration tool, that allows users to examine and edit the features of individual data points in their dataset. It provides multiple views for different modalities such as text, audio, video, and 3D geometry data, enabling users to explore data points in depth and make changes directly within the view. The Inspector is an essential tool for working with multimodal datasets and gives users greater control over the structure and quality of their data. The open-source version of Spotlight allows users to inspect their data, while the Pro version also enables data tagging and annotation. Users can configure the inspector view to choose how each datapoint should be represented, with main configuration options available in the upper right corner of the view. Visualization and interaction components are available for many different data types, including text data, categorical data, image data, audio data, and more. The Inspector plays together with other components in Spotlight to provide a comprehensive data exploration experience.","7":" Sliceguard is a library used to identify data segments where machine learning models perform anomalously, also known as data slices. It helps in finding model failure modes and problematic data segments by interactively exploring these data slices. Sliceguard can be used with Spotlight, which is a data exploration tool for visualizing and understanding machine learning models. Together, they provide a comprehensive solution for data-centric AI, allowing users to diagnose issues in their models by exploring the data that causes those issues.","8":" Spotlight offers two possibilities for representing audio data: an Audio Player and a Spectrogram view. The Audio Player supports annotating event windows in the data, while the Spectrogram provides a visual representation of the audio's frequency components over time.","9":" Renumics Spotlight offers various ways to customize data visualization. The main configuration options for the Inspector View, which allows you to examine and edit individual data points, can be found in the upper right corner of the view. Additionally, you can create custom interaction templates for your team and save preferred layouts for future use. Spotlight includes several UI components such as the Inspector, Data Table, Similarity Map, Scatter Plot, Histogram, and Filter Bar to aid in analyzing and understanding data. For more details on these components and their usage, please refer to the UI components section in the documentation.","10":" Renumics Spotlight allows data to be loaded into it either from memory (as Python object) or from files. Currently supported formats include Pandas dataframes, Huggingface datasets, and Renumics' HDF5-based dataset format. Unstructured data samples can be stored directly in the dataframe or as a path to a separate file. Data can be loaded using the Python API or the command line interface (CLI). For example, data can be loaded from a CSV file using `pd.read_csv` in Python or via the CLI using `curl` and `spotlight` command. Huggingface datasets can be loaded directly into Spotlight with just a few lines of code due to their rich semantic description of feature columns. Additionally, enriched datasets with custom visualization layouts can be explored by joining raw datasets with separate dataframes containing model results such as predictions or embeddings."},"contexts":{"0":["`similaritymap(name=None, columns=None, reduction_method=None, color_by_column=None, size_by_column=None, filter=False, *, umap_metric=None, umap_balance=None, pca_normalization=None)` {similaritymap()}\n\nAdd configured similarity map to Spotlight layout.","---\ntags: []\nid: similarity-map\nsidebar_position: 30\nslug: \/docs\/custom-visualizations\/ui-components\/similarity-map\n---\n\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Similarity Map\n\nThe Similarity Map projects the datapoints onto a 2D Map based on the similarity of the selected columns to place by. The Similarity is measured by either pre computed descriptions (e.G. embeddings retrieved from a ML workflow) or based on one or multiple scalar columns in the dataset.\n\n## Placement\n\nSimply select one ore more columns to \u2018placeBy\u2019 in the settings and watch the dimension reduction arranging the data on the map.\n\nIn order to compute the dimension reduction we offer two different algorithms:\n[UMAP](https:\/\/arxiv.org\/abs\/1802.03426) and [PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis).\nIf the columns values have different distributions,\nThe reduction might run into problems and overestimate the impact of some columns.\nTo counteract this problem, a robust distance metric is available in the advanced settings.\n\nPer default only reduced settings are available to control UMAP\nwhich help you in configuring the reduction to place the samples with more weight on local vs. global similarities.\nHowever, there also are advanced settings available in order give more control over the dimension reduction.\n\nAt the moment the samples can be placed by number columns and Embedding\/Array columns.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/placement\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - place datapoints on the **similarity map** based on a column\"\n\/>\n\n## Controls\n\nThe map can be moved and zoomed by using the mouse.\n\nZooming can be done with the mousewheel, resetting zoom can be done with the `Fit points` button.\n\nThe map can be moved by clicking the `middle mouse` button or with the `left mouse` button if the `alt` key is pressed simultaneously.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/controls\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - navigate in the **similarity map**\"\n\/>\n\n## Selection\n\nSimilar to the [data table](data-table), a single row can be added or removed from the selection.\n\nIn order to select a single point simply click on it. This will reset any selection and exclusively select the clicked point.\n\nTo add a point to a selection click on it while pressing `shift` and to remove it press `ctrl` while clicking on it.\n\nThese steps also apply for multiple points. By pressing the left mouse button and moving the mouse,\na selection rectangle will be shown. On releasing the left mouse button, the selection will be applied in the same fashion as it is done for a single point.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/selection\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - select and deselect points in the **similarity map**\"\n\/>\n\n## Coloring and Sizing\n\nColoring and sizing can be used to represent more columns on the map. The coloring palette can be altered in the global coloring settings.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/coloring-scaling\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - colorize and scale datapoints on the **similarity map** based on a column\"\n\/>\n\n## Filtering\n\nPer default the similarity map also accounts for similarities of filtered out points.\nIn order to compute the similarities only on the filtered points,\n`hide unfiltered` has to be checked.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/filtering\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - show\/hide filtered datapoints on the **similarity map**\"\n\/>"],"1":["`inspector(name=None, lenses=None, num_columns=4)` {inspector()}\n\nAdd an inspector widget with optionally preconfigured viewers (lenses).\n\n Example\n\n```python\n>>> from renumics.spotlight import layout\n>>> from renumics.spotlight.layout import lenses\n>>> spotlight_layout = layout.layout(\n...     layout.inspector(\n...         \"My Inspector\",\n...         [\n...             lenses.scalar(\"bool\"),\n...             lenses.scalar(\"float\"),\n...             lenses.scalar(\"str\"),\n...             lenses.scalar(\"datetime\"),\n...             lenses.scalar(\"category\"),\n...             lenses.scalar(\"int\"),\n...             lenses.text(\"str\", name=\"text\"),\n...             lenses.html(\"str\", name=\"HTML (safe)\"),\n...             lenses.html(\"str\", name=\"HTML\", unsafe=True),\n...             lenses.markdown(\"str\", name=\"MD\"),\n...             lenses.array(\"embedding\"),\n...             lenses.array(\"window\"),\n...             lenses.array(\"array\"),\n...             lenses.sequences(\"sequence\"),\n...             lenses.sequences([\"sequence1\", \"sequence2\"], name=\"sequences\"),\n...             lenses.mesh(\"mesh\"),\n...             lenses.image(\"image\"),\n...             lenses.video(\"video\"),\n...             lenses.audio(\"audio\"),\n...             lenses.audio(\"audio\", window_column=\"window\", name=\"windowed audio\"),\n...             lenses.spectrogram(\"audio\"),\n...             lenses.spectrogram(\n...                 \"audio\",\n...                 window_column=\"window\",\n...                 name=\"windowed spectrogram\",\n...             ),\n...         ],\n...         num_columns=2,\n...     )\n... )\n```","---\nslug: \/docs\/getting-started\nsidebar_position: 10\n---\n\nimport ReactPlayer from \"react-player\";\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\n\n# \ud83d\ude80 Getting Started\n\n> Spotlight helps you to **understand unstructured datasets** fast. You can create **interactive visualizations** from your dataframe with just a few lines of code. You can also leverage data enrichments (e.g. embeddings, prediction, uncertainties) to **identify critical clusters** in your data.\n\n![Dataframe example](\/img\/speech_commands_vis_s.gif)\n\n## What you'll need\n\n-   [Python](https:\/\/www.python.org\/downloads\/) version 3.8-3.11\n\n## Install Spotlight via [pip](https:\/\/packaging.python.org\/en\/latest\/key_projects\/#pip)\n\n```bash\npip install renumics-spotlight\n```\n\n> We recommend installing Spotlight and everything you need to work on your data in a separate [virtual environment](https:\/\/docs.python.org\/3\/tutorial\/venv.html)\n\n## Load your first dataset\n\nData can be loaded into Spotlight either from memory (as Python object) or from file. We currently support Pandas dataframes as well as Huggingface datasets and our own HDF5-based dataset format. Depending on the format, unstructured data samples are either stored directly in the dataframe or as a path to a separate file.\n\nThis is an example how your dataframe might look like:\n\n![Dataframe example](\/img\/dataframe_head_sample.png)\n\nYou can directly load your dataframe either via the Python API or the command line interface (CLI):\n\n<Tabs groupId=\"interface\">\n<TabItem value=\"python\" label=\"python\" default>\n\n```python\nimport pandas as pd\nfrom renumics import spotlight\n\ndf = pd.read_csv(\"https:\/\/renumics.com\/data\/mnist\/mnist-tiny.csv\")\nspotlight.show(df, dtype={\"image\": spotlight.Image, \"embedding\": spotlight.Embedding})\n```\n\n-   `pd.read_csv` loads a sample csv file as a pandas [DataFrame](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html).\n-   `spotlight.show` opens up spotlight in the browser with the pandas dataframe ready for you to explore.\n-   The `dtype` argument specifies custom column types for the browser viewer.\n\n<\/TabItem>\n<TabItem value=\"bash\" label=\"CLI\">\n\n```bash\ncurl https:\/\/renumics.com\/data\/mnist\/mnist-tiny.csv -o mnist-tiny.csv\nspotlight mnist-tiny.csv --dtype image=Image --dtype embedding=Embedding\n```\n\n<\/TabItem>\n<\/Tabs>\n\n## Load a [Hugging Face](https:\/\/huggingface.co\/) dataset\n\nHuggingface datasets have a rich semantic description of the feature columns. Spotlight can thus parse data type descriptions and label mappings automatically. This means that creating a visualization is typically as simple as:\n\n```python\nimport datasets\nfrom renumics import spotlight\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\n\nspotlight.show(ds)\n```\n\n## Exploring an enriched dataset with custom visualization layouts\n\nExploring raw unstructured datasets often yield little insights. Leveraging model results such as predictions or embeddings can help to uncover critical data samples and clusters. In practice, these enrichments can be stored in a separate dataframe and then joined with the raw dataset. Here is an example from the Huggingface hub:\n\n```python\nimport datasets\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\nds_results = datasets.load_dataset('renumics\/speech_commands-ast-finetuned-results', 'v0.01', split='all')\nds = datasets.concatenate_datasets([ds, ds_results], axis=1)\n```\n\nDepending on the task at hand (e.g. EDA, model debugging, monitoring), you probably want to set up a suitable visualization layout. You can do so in the GUI and via API."],"2":["---\ntags: []\nid: duplicates-annoy\nsidebar_position: 35\nslug: \/docs\/data-centric-ai\/playbook\/duplicates-annoy\n---\n\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Detect duplicates with Annoy\n\nWe use embeddings to detect duplicates by computing nearest neighbors with the Annoy library. Although the example is based on image embeddings, the basic play is independent of the data type.\n\n> Use Chrome to run Spotlight in Colab. Due to Colab restrictions (e.g. no websocket support), the performance is limited. Run the notebook locally for the full Spotlight experience.\n\n<a\n    target=\"_blank\"\n    href=\"https:\/\/colab.research.google.com\/github\/Renumics\/spotlight\/blob\/main\/playbook\/veteran\/duplicates_annoy.ipynb\"\n>\n    <img\n        src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\"\n        alt=\"Open In Colab\"\n    \/>\n<\/a>\n\n<Tabs groupId=\"usedDatasets\" >\n<TabItem value=\"inputs\" label=\"inputs\">\n\n-   `df['embedding']` contain the [embeddings](..\/glossary\/index.mdx#embedding) for each data sample\n\n<\/TabItem>\n<TabItem value=\"outputs\" label=\"outputs\">\n\n-   `df['nn_id']` contains the sample id for the [nearest neighbor](..\/glossary\/index.mdx#nearest-neighbor) in the embedding space.\n-   `df['nn_image']` contains the path to the [image](..\/glossary\/index.mdx#image-data) that is the nearest neighbor.\n-   `df['nn_distance']` contains distance to the nearest neighbor.\n-   `df['nn_flag']` contains a flag that indicates if the sample is a near-duplicate according to the given threshold.\n\n<\/TabItem>\n<TabItem value=\"parameters\" label=\"parameters\">\n* `threshold` denotes the distance threshold when a data sample is considered a near-duplicate.\n* `tree_size` is an internal parameter for the Annoy library that calibrates the speed-efficiency tradeoff. More trees gives higher precision when querying.\n<\/TabItem>\n<\/Tabs>\n\n![Spotlight_screenshot_decision_boundary](\/playbook\/duplicates_annoy_screenshot.png)\n\n## Imports and play as copy-n-paste functions\n\n<details>\n    <summary># Install dependencies<\/summary>\n\n```python\n#@title Install required packages with PIP\n\n!pip install renumics-spotlight datasets annoy\n```\n\n<\/details>\n\n<details>\n    <summary># Play as copy-n-paste functions<\/summary>\n\n```python\n#@title Play as copy-n-paste functions\n\nimport datasets\nfrom renumics import spotlight\nfrom annoy import AnnoyIndex\nimport pandas as pd\nimport requests\n\n\ndef nearest_neighbor_annoy(df, embedding_name='embedding', threshold=0.3, tree_size=100):\n\n    embs = df[embedding_name]\n\n    t = AnnoyIndex(len(embs[0]), 'angular')\n\n    for idx, x in enumerate(embs):\n          t.add_item(idx, x)\n\n    t.build(tree_size)\n\n    images = df['image']\n\n    df_nn = pd.DataFrame()\n\n    nn_id = [t.get_nns_by_item(i,2)[1] for i in range(len(embs))]\n    df_nn['nn_id'] = nn_id\n    df_nn['nn_image'] = [images[i] for i in nn_id]\n    df_nn['nn_distance'] = [t.get_distance(i, nn_id[i]) for i in range(len(embs))]\n    df_nn['nn_flag'] = (df_nn.nn_distance < threshold)\n\n\n    return df_nn\n```\n\n<\/details>\n\n## Step-by-step example on CIFAR-100\n\n### Load CIFAR-100 from Huggingface hub and convert it to Pandas dataframe\n\n```python\ndataset = datasets.load_dataset(\"renumics\/cifar100-enriched\", split=\"train\")\ndf = dataset.to_pandas()\n```\n\n### Compute nearest neighbors including distances\n\n```python\ndf_nn = nearest_neighbor_annoy(df)\ndf = pd.concat([df, df_nn], axis=1)\n```\n\n### Inspect and remove duplicates with Spotlight\n\n```python\ndf_show = df.drop(columns=['embedding', 'probabilities'])\nlayout_url = \"https:\/\/raw.githubusercontent.com\/Renumics\/spotlight\/playbook_initial_draft\/playbook\/rookie\/duplicates_annoy.json\"\nresponse = requests.get(layout_url)\nlayout = spotlight.layout.nodes.Layout(**json.loads(response.text))\nspotlight.show(df_show, dtype={\"image\": spotlight.Image, \"embedding_reduced\": spotlight.Embedding}, layout=layout)\n```","---\ntags: []\nid: text-classification\nslug: \/docs\/use-cases\/text-classification\n---\n\n# Detect Problems in Text Classification Datasets\n\nUse the [**sliceguard**](https:\/\/github.com\/Renumics\/sliceguard) library and [**Spotlight**](https:\/\/github.com\/Renumics\/spotlight) to quickly **detect problem clusters** that can cause issues when training your text classification model. Shows problems such as:\n\n1. Label Inconsistencies\n2. Outliers and Errors\n3. Text-specific Issues (e.g. typos, empty samples)\n\nGenerally it will show you everything that is hard to learn for a classification model. All you need is a dataframe containing **texts** and **labels**.\n\nFirst install the dependencies:\n\n```\npip install renumics-spotlight sliceguard[all] scikit-learn\n```\n\nThen run the following code to detect problematic clusters:\n\n```python\n# The Imports\nfrom renumics import spotlight\nfrom sliceguard import SliceGuard\nfrom sliceguard.data import from_huggingface\nfrom sklearn.metrics import accuracy_score\n\n# Load an Example Dataset as DataFrame\ndf = from_huggingface(\"dair-ai\/emotion\")\n\n# DataFrame Format:\n# +-------+-------+\n# | text  | label |\n# +-------+-------+\n# | text1 | joy   |\n# | text2 | anger |\n# | text3 | joy   |\n# | ...   |       |\n# +-------+-------+\n\n# Detect Issues Using sliceguard\nsg = SliceGuard()\nissues = sg.find_issues(df, features=[\"text\"], y=\"label\", metric=accuracy_score)\nreport_df, spotlight_data_issues, spotlight_dtypes, spotlight_layout = sg.report(\n    no_browser=True\n)\n\n# Visualize Detected Issues in Spotlight:\nspotlight.show(\n    report_df,\n    dtype=spotlight_dtypes,\n    issues=spotlight_data_issues,\n    layout=spotlight_layout,\n)\n```"],"3":["---\nsidebar_position: 30\nslug: \/docs\/configure-visualizations\n---\n\n# Configuring visualizations\n\nData inspection is critical for all steps in the ML development workflow. But it is also time consuming: First of all it takes effort to build and maintain custom visualization pipelines. Second of all, it takes time to sift through data manually.\n\nSpotlight enables you to build scalable data inspection workflows. With Spotlight you can\n\n-   Create visualizations for unstructured data with a single line of code\n-   Leverage model results to find relevant data segments quickly\n-   Create custom interaction templates for your team\n\nIn order to achieve this, the Spotlight UI provides a range of components that offer different perspectives on your data. You can add, remove, and rearrange these components as you wish to create a customized inspection layouts.\n\nOnce you have identified a component layout that works well for you, you can save it as a layout for future use. This way, you can quickly load your preferred layout whenever you need it, without having to reconfigure the UI each time.\n\n## UI components\n\nSpotlight includes the [Inspector](ui_components\/inspector_view.mdx) and the [Data Table](ui_components\/table_view.mdx) to help you in analyzing and understanding individual datapoints. Other visualization options like the [Similarity Map](ui_components\/similarity_map.mdx), the [Scatter Plot](ui_components\/scatter_plot.mdx) and the [Histogram](ui_components\/histogram.mdx) aide you to understand the overall distribution of your data.\n\nThe [Filter Bar](ui_components\/filter_bar.mdx) is a special component that allows you to filter your data based on the values of your features.<br\/>\nFiltering and selecting datapoints can affect how data is presented in the other components and therefore can greatly help you in analyzing and reasoning about your data.\n\nAn overview over all available components can be found in the [UI components](ui_components\/index.mdx) section.\n\n## Layout API\n\n## Issues widget","---\nsidebar_position: 40\nslug: \/docs\/configure-visualizations\/issues\n---\n\n# \ud83d\udd16 Defining data issues\n\nSpotlight allows to define highlight specific data clusters. These clusters can be defined by their respective row and\/or column ids an allow to quickly inspect critical issues in your data."],"4":["---\ntags: []\nid: audio-classification\nslug: \/docs\/use-cases\/audio-classification\n---\n\n# Detect Problems in Audio Classification Datasets\n\nUse the [**sliceguard**](https:\/\/github.com\/Renumics\/sliceguard) library and [**Spotlight**](https:\/\/github.com\/Renumics\/spotlight) to quickly **detect problem clusters** that can cause issues when training your audio classification model. Shows problems such as:\n\n1. Label Inconsistencies\n2. Outliers and Errors\n3. Audio-specific Issues (e.g. clipping)\n\nGenerally it will show you everything that is hard to learn for a classification model. All you need is a dataframe containing **paths to audio files** and **labels**.\n\nFirst install the dependencies:\n\n```\npip install renumics-spotlight sliceguard[all] scikit-learn\n```\n\nThen run the following code to detect problematic clusters:\n\n```python\n# The Imports\nfrom renumics import spotlight\nfrom sliceguard import SliceGuard\nfrom sliceguard.data import from_huggingface\nfrom sklearn.metrics import accuracy_score\n\n# Load an Example Dataset as DataFrame\ndf = from_huggingface(\"renumics\/emodb\")\n\n# DataFrame Format:\n# +---------------------+---------+\n# |        audio        | emotion |\n# +---------------------+---------+\n# | \/path\/to\/audio1.wav | joy     |\n# | \/path\/to\/audio2.wav | anger   |\n# | \/path\/to\/audio3.wav | joy     |\n# | ...                 |         |\n# +---------------------+---------+\n\n# Detect Issues Using sliceguard\nsg = SliceGuard()\nissues = sg.find_issues(df, features=[\"audio\"], y=\"emotion\", metric=accuracy_score)\nreport_df, spotlight_data_issues, spotlight_dtypes, spotlight_layout = sg.report(\n    no_browser=True\n)\n\n# Visualize Detected Issues in Spotlight:\nspotlight.show(\n    report_df,\n    dtype=spotlight_dtypes,\n    issues=spotlight_data_issues,\n    layout=spotlight_layout,\n)\n\n```","<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/image-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/image-view-512.jpg\"\n    annotation=\"_mnist_ - display and zoom\/pan images in a **Image** view\"\n\/>\n\n### Audio data\n\n**Audio data** can be represented with an _Audio Player_ or a _Spectrogram_. The _Audio Player_ supports annotating event windows in the data.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/audio-window-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/audio-window-view-512.jpg\"\n    annotation=\"_fsd50k_ - display and control audio with\/without an additional window\"\n\/>\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/audio-window-edit-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/audio-window-edit-view-512.jpg\"\n    annotation=\"_fsd50k_ - edit audio window\"\n\/>\n\n### Video data\n\n**Video data** can be represented with an _Video Player_.\n\n### 3D Meshes\n\n**3D meshes** can be represented by a _Mesh Viewer_. This components supports surface coloring to display mesh properties (e.g. stresses) and animated meshes.\n\n### Boolean data\n\n**Boolean data** can be represented by a _Switch_ element to facilitate efficient data annotation and tagging.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/bool-switch-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/bool-switch-view-512.jpg\"\n    annotation=\"_fsd50k_ - edit bool values in **Switch** view\"\n\/>"],"5":["---\nsidebar_position: 30\nslug: \/docs\/configure-visualizations\n---\n\n# Configuring visualizations\n\nData inspection is critical for all steps in the ML development workflow. But it is also time consuming: First of all it takes effort to build and maintain custom visualization pipelines. Second of all, it takes time to sift through data manually.\n\nSpotlight enables you to build scalable data inspection workflows. With Spotlight you can\n\n-   Create visualizations for unstructured data with a single line of code\n-   Leverage model results to find relevant data segments quickly\n-   Create custom interaction templates for your team\n\nIn order to achieve this, the Spotlight UI provides a range of components that offer different perspectives on your data. You can add, remove, and rearrange these components as you wish to create a customized inspection layouts.\n\nOnce you have identified a component layout that works well for you, you can save it as a layout for future use. This way, you can quickly load your preferred layout whenever you need it, without having to reconfigure the UI each time.\n\n## UI components\n\nSpotlight includes the [Inspector](ui_components\/inspector_view.mdx) and the [Data Table](ui_components\/table_view.mdx) to help you in analyzing and understanding individual datapoints. Other visualization options like the [Similarity Map](ui_components\/similarity_map.mdx), the [Scatter Plot](ui_components\/scatter_plot.mdx) and the [Histogram](ui_components\/histogram.mdx) aide you to understand the overall distribution of your data.\n\nThe [Filter Bar](ui_components\/filter_bar.mdx) is a special component that allows you to filter your data based on the values of your features.<br\/>\nFiltering and selecting datapoints can affect how data is presented in the other components and therefore can greatly help you in analyzing and reasoning about your data.\n\nAn overview over all available components can be found in the [UI components](ui_components\/index.mdx) section.\n\n## Layout API\n\n## Issues widget","---\ntags: []\nid: inspector\nsidebar_position: 20\nslug: \/docs\/custom-visualizations\/ui-components\/inspector\n---\n\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Inspector\n\nThe Inspector Widget is a component in Spotlight that lets you examine and edit the features of **individual data points** in your dataset.\nWith its multiple views for **different modalities**, the Inspector Widget provides a detailed look at a data point's underlying structure.\n\nThe Inspector Widget's views for text, audio, video, and 3D geometry data allow you to explore a data point's features in depth.\nYou can also edit those features directly within the view, making it easy to make changes to a data point without navigating to a separate screen.\nThe Inspector Widget is an essential tool for working with multimodal datasets and gives you greater control over the structure and quality of your data.\n\n:::info\n\nThe open source version of Renumics Spotlight allows you to inspect your data to find critical segments. The Pro version of Spotlight also enables data tagging and annotation.\n\n:::\n\nBy configuring the inspector view you can choose how each datapoint should be represented. The main configuration options are available in the upper right corner of the view:\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/inspector-view-overview\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720, 1280]}\n    annotation=\"_fsd50k_ - add and layout components in the **Inspector View**\"\n\/>\n\nThere are visualization and interaction components available for many different data types. These include:\n\n### Text data\n\n**Text data** can be represented either with a **Value** view or a **Select** view that features autocomplete.\n\n<!-- poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\/text-value-field.jpg\" -->\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field-512.jpg\"\n    annotation=\"_fsd50k_ - display annotation in a **Value** view\"\n\/>\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field.jpg\"\n    annotation=\"_fsd50k_ - display and update annotation in a **Select** view\"\n\/>\n\n### Categorical data\n\n**Categorical data** is represented similarly to **Text data** with the exception, that when there are not too many categories, they can also be edited with a switch component.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field-512.jpg\"\n    annotation=\"_mnist_ - display and update categories in a **Switch** view\"\n\/>\n\n### Image data\n\n**Image data** can be represented with an imager viewer that supports zooming and panning.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/image-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/image-view-512.jpg\"\n    annotation=\"_mnist_ - display and zoom\/pan images in a **Image** view\"\n\/>\n\n### Audio data\n\n**Audio data** can be represented with an _Audio Player_ or a _Spectrogram_. The _Audio Player_ supports annotating event windows in the data."],"6":["---\ntags: []\nid: inspector\nsidebar_position: 20\nslug: \/docs\/custom-visualizations\/ui-components\/inspector\n---\n\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Inspector\n\nThe Inspector Widget is a component in Spotlight that lets you examine and edit the features of **individual data points** in your dataset.\nWith its multiple views for **different modalities**, the Inspector Widget provides a detailed look at a data point's underlying structure.\n\nThe Inspector Widget's views for text, audio, video, and 3D geometry data allow you to explore a data point's features in depth.\nYou can also edit those features directly within the view, making it easy to make changes to a data point without navigating to a separate screen.\nThe Inspector Widget is an essential tool for working with multimodal datasets and gives you greater control over the structure and quality of your data.\n\n:::info\n\nThe open source version of Renumics Spotlight allows you to inspect your data to find critical segments. The Pro version of Spotlight also enables data tagging and annotation.\n\n:::\n\nBy configuring the inspector view you can choose how each datapoint should be represented. The main configuration options are available in the upper right corner of the view:\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/inspector-view-overview\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720, 1280]}\n    annotation=\"_fsd50k_ - add and layout components in the **Inspector View**\"\n\/>\n\nThere are visualization and interaction components available for many different data types. These include:\n\n### Text data\n\n**Text data** can be represented either with a **Value** view or a **Select** view that features autocomplete.\n\n<!-- poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\/text-value-field.jpg\" -->\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field-512.jpg\"\n    annotation=\"_fsd50k_ - display annotation in a **Value** view\"\n\/>\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field.jpg\"\n    annotation=\"_fsd50k_ - display and update annotation in a **Select** view\"\n\/>\n\n### Categorical data\n\n**Categorical data** is represented similarly to **Text data** with the exception, that when there are not too many categories, they can also be edited with a switch component.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field-512.jpg\"\n    annotation=\"_mnist_ - display and update categories in a **Switch** view\"\n\/>\n\n### Image data\n\n**Image data** can be represented with an imager viewer that supports zooming and panning.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/image-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/image-view-512.jpg\"\n    annotation=\"_mnist_ - display and zoom\/pan images in a **Image** view\"\n\/>\n\n### Audio data\n\n**Audio data** can be represented with an _Audio Player_ or a _Spectrogram_. The _Audio Player_ supports annotating event windows in the data.","`inspector(name=None, lenses=None, num_columns=4)` {inspector()}\n\nAdd an inspector widget with optionally preconfigured viewers (lenses).\n\n Example\n\n```python\n>>> from renumics.spotlight import layout\n>>> from renumics.spotlight.layout import lenses\n>>> spotlight_layout = layout.layout(\n...     layout.inspector(\n...         \"My Inspector\",\n...         [\n...             lenses.scalar(\"bool\"),\n...             lenses.scalar(\"float\"),\n...             lenses.scalar(\"str\"),\n...             lenses.scalar(\"datetime\"),\n...             lenses.scalar(\"category\"),\n...             lenses.scalar(\"int\"),\n...             lenses.text(\"str\", name=\"text\"),\n...             lenses.html(\"str\", name=\"HTML (safe)\"),\n...             lenses.html(\"str\", name=\"HTML\", unsafe=True),\n...             lenses.markdown(\"str\", name=\"MD\"),\n...             lenses.array(\"embedding\"),\n...             lenses.array(\"window\"),\n...             lenses.array(\"array\"),\n...             lenses.sequences(\"sequence\"),\n...             lenses.sequences([\"sequence1\", \"sequence2\"], name=\"sequences\"),\n...             lenses.mesh(\"mesh\"),\n...             lenses.image(\"image\"),\n...             lenses.video(\"video\"),\n...             lenses.audio(\"audio\"),\n...             lenses.audio(\"audio\", window_column=\"window\", name=\"windowed audio\"),\n...             lenses.spectrogram(\"audio\"),\n...             lenses.spectrogram(\n...                 \"audio\",\n...                 window_column=\"window\",\n...                 name=\"windowed spectrogram\",\n...             ),\n...         ],\n...         num_columns=2,\n...     )\n... )\n```"],"7":["---\ntags: []\nid: sliceguard\nsidebar_position: 6055\nslug: \/docs\/data-centric-ai\/playbook\/data-slices-sliceguard\n---\n\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Find data slices with Sliceguard\n\nWe use [Sliceguard](https:\/\/github.com\/Renumics\/sliceguard) to identify data segments where our machine learning model performs anomalously (data slices). We interactively explore these data slices to find model failure modes and problematic data segments.\n\n> Use Chrome to run Spotlight in Colab. Due to Colab restrictions (e.g. no websocket support), the performance is limited. Run the notebook locally for the full Spotlight experience.\n\n<a\n    target=\"_blank\"\n    href=\"https:\/\/colab.research.google.com\/github\/Renumics\/spotlight\/blob\/main\/playbook\/allstar\/data-slices-sliceguard.ipynb\"\n>\n    <img\n        src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\"\n        alt=\"Open In Colab\"\n    \/>\n<\/a>\n\n<Tabs groupId=\"usedDatasets\" >\n<TabItem value=\"inputs\" label=\"inputs\">\n\n-   `categories` contains contain the names of the [features](..\/glossary\/index.mdx#features) and [metadata](..\/glossary\/index.mdx#metadata) columns that are to be analyzed\n-   `df['label']` contains the [label](..\/glossary\/index.mdx#label) for each data sample\n-   `df['prediction']` contains the [prediction](..\/glossary\/index.mdx#label) for each data sample\n-   `df['embedding']` contains the [embeddings](..\/glossary\/index.mdx#embedding) for each data sample (optional)\n\n<\/TabItem>\n\n<TabItem value=\"outputs\" label=\"outputs\">\n\n-   `df_slices contains a dataframe with a description of the issues found\n\n<\/TabItem>\n\n<TabItem value=\"parameters\" label=\"parameters\">\n* `category_types` (optional) describes the type of the features and metadata (\"raw\", \"nominal\", \"ordinal\", \"numerical\", \"embedding\").\n* `spotlight_dtype` (optional) describes data types for the visualization with Spotlight.\n<\/TabItem>\n<\/Tabs>\n\n![Spotlight_screenshot_drift_kcore](\/playbook\/screenshot_sliceguard.png)\n\n## Imports and play as copy-n-paste functions\n\n<details>\n    <summary># Install dependencies<\/summary>\n\n```python\n#@title Install required packages with PIP\n\n!pip install renumics-spotlight sliceguard datasets cleanvision\n```\n\n<\/details>\n\n<details>\n    <summary>Play as copy-n-paste functions<\/summary>\n\n```python\n#@title Play as copy-n-paste snippet\n\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport datasets\nfrom renumics.spotlight import Image\nfrom sliceguard import SliceGuard\nfrom cleanvision.imagelab import Imagelab","def find_data_slices(df, categories, category_types={}, spotlight_dtype={}, embedding_name='embedding', label_name='label', prediction_name='prediction'):\n    sg = SliceGuard()\n    df_slices = sg.find_issues(\n        df,\n        categories,\n        label_name,\n        prediction_name,\n        accuracy_score,\n        precomputed_embeddings = {'embedding': df[embedding_name].to_numpy()},\n        metric_mode=\"max\",\n        feature_types=category_types\n    )\n\n    sg.report(spotlight_dtype=spotlight_dtype)\n\n    return df_slices\n```\n\n<\/details>\n\n## Step-by-step example on CIFAR-100\n\n### Load CIFAR-100 from Huggingface hub and convert it to Pandas dataframe\n\n```python\ndataset = datasets.load_dataset(\"renumics\/cifar100-enriched\", split=\"test\")\ndf = dataset.to_pandas()\n```\n\n### Enrich dataset with metadata using [Cleanvision](.\/cv_issues.mdx)\n\n```python\ndef cv_issues_cleanvision(df, image_name='image'):\n\n    image_paths = df['image'].to_list()\n    imagelab = Imagelab(filepaths=image_paths)\n    imagelab.find_issues()\n\n    df_cv=imagelab.issues.reset_index()\n\n    return df_cv\n\ndf_cv = cv_issues_cleanvision(df)\ndf = pd.concat([df, df_cv], axis=1)\n```\n\n### Identify and explore data slices with Sliceguard\n\n```python\ncategories=['dark_score', 'low_information_score', 'light_score', 'blurry_score', 'fine_label']\nprediction = 'fine_label_prediction'\nlabel = 'fine_label'\ncategory_types={'fine_label': 'nominal'}\nspotlight_dtype={\"image\": Image}\n\nfind_data_slices(df, categories, category_types=category_types, spotlight_dtype=spotlight_dtype, embedding_name='embedding', label_name=label, prediction_name=prediction)\n\n```"],"8":["`audio(column, window_column=None, name=None, *, repeat=False, autoplay=False)` {audio()}\n\nAdd audio viewer to Spotlight inspector widget.\n\nSupports a single column of type `spotlight.Audio` with optional second\ncolumn of type `spotlight.Window`.","---\ntags: []\nid: inspector\nsidebar_position: 20\nslug: \/docs\/custom-visualizations\/ui-components\/inspector\n---\n\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Inspector\n\nThe Inspector Widget is a component in Spotlight that lets you examine and edit the features of **individual data points** in your dataset.\nWith its multiple views for **different modalities**, the Inspector Widget provides a detailed look at a data point's underlying structure.\n\nThe Inspector Widget's views for text, audio, video, and 3D geometry data allow you to explore a data point's features in depth.\nYou can also edit those features directly within the view, making it easy to make changes to a data point without navigating to a separate screen.\nThe Inspector Widget is an essential tool for working with multimodal datasets and gives you greater control over the structure and quality of your data.\n\n:::info\n\nThe open source version of Renumics Spotlight allows you to inspect your data to find critical segments. The Pro version of Spotlight also enables data tagging and annotation.\n\n:::\n\nBy configuring the inspector view you can choose how each datapoint should be represented. The main configuration options are available in the upper right corner of the view:\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/inspector-view-overview\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720, 1280]}\n    annotation=\"_fsd50k_ - add and layout components in the **Inspector View**\"\n\/>\n\nThere are visualization and interaction components available for many different data types. These include:\n\n### Text data\n\n**Text data** can be represented either with a **Value** view or a **Select** view that features autocomplete.\n\n<!-- poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\/text-value-field.jpg\" -->\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field-512.jpg\"\n    annotation=\"_fsd50k_ - display annotation in a **Value** view\"\n\/>\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field.jpg\"\n    annotation=\"_fsd50k_ - display and update annotation in a **Select** view\"\n\/>\n\n### Categorical data\n\n**Categorical data** is represented similarly to **Text data** with the exception, that when there are not too many categories, they can also be edited with a switch component.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field-512.jpg\"\n    annotation=\"_mnist_ - display and update categories in a **Switch** view\"\n\/>\n\n### Image data\n\n**Image data** can be represented with an imager viewer that supports zooming and panning.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/image-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/image-view-512.jpg\"\n    annotation=\"_mnist_ - display and zoom\/pan images in a **Image** view\"\n\/>\n\n### Audio data\n\n**Audio data** can be represented with an _Audio Player_ or a _Spectrogram_. The _Audio Player_ supports annotating event windows in the data."],"9":["---\ntags: []\nid: inspector\nsidebar_position: 20\nslug: \/docs\/custom-visualizations\/ui-components\/inspector\n---\n\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Inspector\n\nThe Inspector Widget is a component in Spotlight that lets you examine and edit the features of **individual data points** in your dataset.\nWith its multiple views for **different modalities**, the Inspector Widget provides a detailed look at a data point's underlying structure.\n\nThe Inspector Widget's views for text, audio, video, and 3D geometry data allow you to explore a data point's features in depth.\nYou can also edit those features directly within the view, making it easy to make changes to a data point without navigating to a separate screen.\nThe Inspector Widget is an essential tool for working with multimodal datasets and gives you greater control over the structure and quality of your data.\n\n:::info\n\nThe open source version of Renumics Spotlight allows you to inspect your data to find critical segments. The Pro version of Spotlight also enables data tagging and annotation.\n\n:::\n\nBy configuring the inspector view you can choose how each datapoint should be represented. The main configuration options are available in the upper right corner of the view:\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/inspector-view-overview\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720, 1280]}\n    annotation=\"_fsd50k_ - add and layout components in the **Inspector View**\"\n\/>\n\nThere are visualization and interaction components available for many different data types. These include:\n\n### Text data\n\n**Text data** can be represented either with a **Value** view or a **Select** view that features autocomplete.\n\n<!-- poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\/text-value-field.jpg\" -->\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field-512.jpg\"\n    annotation=\"_fsd50k_ - display annotation in a **Value** view\"\n\/>\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field.jpg\"\n    annotation=\"_fsd50k_ - display and update annotation in a **Select** view\"\n\/>\n\n### Categorical data\n\n**Categorical data** is represented similarly to **Text data** with the exception, that when there are not too many categories, they can also be edited with a switch component.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field-512.jpg\"\n    annotation=\"_mnist_ - display and update categories in a **Switch** view\"\n\/>\n\n### Image data\n\n**Image data** can be represented with an imager viewer that supports zooming and panning.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/image-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/image-view-512.jpg\"\n    annotation=\"_mnist_ - display and zoom\/pan images in a **Image** view\"\n\/>\n\n### Audio data\n\n**Audio data** can be represented with an _Audio Player_ or a _Spectrogram_. The _Audio Player_ supports annotating event windows in the data.","---\nsidebar_position: 30\nslug: \/docs\/configure-visualizations\n---\n\n# Configuring visualizations\n\nData inspection is critical for all steps in the ML development workflow. But it is also time consuming: First of all it takes effort to build and maintain custom visualization pipelines. Second of all, it takes time to sift through data manually.\n\nSpotlight enables you to build scalable data inspection workflows. With Spotlight you can\n\n-   Create visualizations for unstructured data with a single line of code\n-   Leverage model results to find relevant data segments quickly\n-   Create custom interaction templates for your team\n\nIn order to achieve this, the Spotlight UI provides a range of components that offer different perspectives on your data. You can add, remove, and rearrange these components as you wish to create a customized inspection layouts.\n\nOnce you have identified a component layout that works well for you, you can save it as a layout for future use. This way, you can quickly load your preferred layout whenever you need it, without having to reconfigure the UI each time.\n\n## UI components\n\nSpotlight includes the [Inspector](ui_components\/inspector_view.mdx) and the [Data Table](ui_components\/table_view.mdx) to help you in analyzing and understanding individual datapoints. Other visualization options like the [Similarity Map](ui_components\/similarity_map.mdx), the [Scatter Plot](ui_components\/scatter_plot.mdx) and the [Histogram](ui_components\/histogram.mdx) aide you to understand the overall distribution of your data.\n\nThe [Filter Bar](ui_components\/filter_bar.mdx) is a special component that allows you to filter your data based on the values of your features.<br\/>\nFiltering and selecting datapoints can affect how data is presented in the other components and therefore can greatly help you in analyzing and reasoning about your data.\n\nAn overview over all available components can be found in the [UI components](ui_components\/index.mdx) section.\n\n## Layout API\n\n## Issues widget"],"10":["---\ntitle: \ud83e\udd17 Loading Huggingface Data\nslug: \/docs\/loading-data\/huggingface\nsidebar_position: 30\n---\n\n# Loading Data from Huggingface datasets\n\n![Huggingface example](\/img\/speech_commands_vis_s.gif)\n\nThe Hugging Face datasets library not only provides access to more than 70k publicly available datasets, but also offers very convenient data preparation pipelines for custom datasets.\n\nRenumics Spotlight allows you to create interactive visualizations to identify critical clusters in your data. Because Spotlight understands the data semantics within Hugging Face datasets, you can get started with just one line of code:\n\n```python\nimport datasets\nfrom renumics import spotlight\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='validation')\n\nspotlight.show(ds)\n```\n\n## Spotlight \ud83e\udd1d Hugging Face datasets\n\nThe datasets library has several features that makes it an ideal tool for working with ML datasets: It stores tabular data (e.g. metadata, labels) along with unstructured data (e.g. images, audio) in a common Arrows table. Datasets also describes important data semantics through features (e.g. images, audio) and additional task-specific metadata.\n\nSpotlight directly works on top of the datasets library. This means that there is no need to copy or pre-process the dataset for data visualization and inspection. Spotlight loads the tabular data into memory to allow for efficient, client-side data analytics. Memory-intensive unstructured data samples (e.g. audio, images, video) are loaded lazily on demand. In most cases, data types and label mappings are inferred directly from the dataset. Here, we visualize the CIFAR-100 dataset with one line of code:\n\n```python\nds = datasets.load_dataset('cifar100', split='test')\nspotlight.show(ds)\n```\n\nIn cases where the data types are ambiguous or not specified, the Spotlight API allows to manually assign them:\n\n```python\nlabel_mapping = dict(zip(ds.features['fine_label'].names, range(len(ds.features['fine_label'].names))))\nspotlight.show(ds, dtype={'img': spotlight.Image, 'fine_label': spotlight.dtypes.CategoryDType(categories=label_mapping)})\n```\n\n## Supported data types\n\nMore details on the supported data types can be found in the [data types section](..\/loading-data#supported-data-types)\n\n## Detailed examples\n\nTake a look at our [use case section](..\/use-cases) to find more detailed examples for different modalities.","---\nslug: \/docs\/getting-started\nsidebar_position: 10\n---\n\nimport ReactPlayer from \"react-player\";\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\n\n# \ud83d\ude80 Getting Started\n\n> Spotlight helps you to **understand unstructured datasets** fast. You can create **interactive visualizations** from your dataframe with just a few lines of code. You can also leverage data enrichments (e.g. embeddings, prediction, uncertainties) to **identify critical clusters** in your data.\n\n![Dataframe example](\/img\/speech_commands_vis_s.gif)\n\n## What you'll need\n\n-   [Python](https:\/\/www.python.org\/downloads\/) version 3.8-3.11\n\n## Install Spotlight via [pip](https:\/\/packaging.python.org\/en\/latest\/key_projects\/#pip)\n\n```bash\npip install renumics-spotlight\n```\n\n> We recommend installing Spotlight and everything you need to work on your data in a separate [virtual environment](https:\/\/docs.python.org\/3\/tutorial\/venv.html)\n\n## Load your first dataset\n\nData can be loaded into Spotlight either from memory (as Python object) or from file. We currently support Pandas dataframes as well as Huggingface datasets and our own HDF5-based dataset format. Depending on the format, unstructured data samples are either stored directly in the dataframe or as a path to a separate file.\n\nThis is an example how your dataframe might look like:\n\n![Dataframe example](\/img\/dataframe_head_sample.png)\n\nYou can directly load your dataframe either via the Python API or the command line interface (CLI):\n\n<Tabs groupId=\"interface\">\n<TabItem value=\"python\" label=\"python\" default>\n\n```python\nimport pandas as pd\nfrom renumics import spotlight\n\ndf = pd.read_csv(\"https:\/\/renumics.com\/data\/mnist\/mnist-tiny.csv\")\nspotlight.show(df, dtype={\"image\": spotlight.Image, \"embedding\": spotlight.Embedding})\n```\n\n-   `pd.read_csv` loads a sample csv file as a pandas [DataFrame](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html).\n-   `spotlight.show` opens up spotlight in the browser with the pandas dataframe ready for you to explore.\n-   The `dtype` argument specifies custom column types for the browser viewer.\n\n<\/TabItem>\n<TabItem value=\"bash\" label=\"CLI\">\n\n```bash\ncurl https:\/\/renumics.com\/data\/mnist\/mnist-tiny.csv -o mnist-tiny.csv\nspotlight mnist-tiny.csv --dtype image=Image --dtype embedding=Embedding\n```\n\n<\/TabItem>\n<\/Tabs>\n\n## Load a [Hugging Face](https:\/\/huggingface.co\/) dataset\n\nHuggingface datasets have a rich semantic description of the feature columns. Spotlight can thus parse data type descriptions and label mappings automatically. This means that creating a visualization is typically as simple as:\n\n```python\nimport datasets\nfrom renumics import spotlight\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\n\nspotlight.show(ds)\n```\n\n## Exploring an enriched dataset with custom visualization layouts\n\nExploring raw unstructured datasets often yield little insights. Leveraging model results such as predictions or embeddings can help to uncover critical data samples and clusters. In practice, these enrichments can be stored in a separate dataframe and then joined with the raw dataset. Here is an example from the Huggingface hub:\n\n```python\nimport datasets\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\nds_results = datasets.load_dataset('renumics\/speech_commands-ast-finetuned-results', 'v0.01', split='all')\nds = datasets.concatenate_datasets([ds, ds_results], axis=1)\n```\n\nDepending on the task at hand (e.g. EDA, model debugging, monitoring), you probably want to set up a suitable visualization layout. You can do so in the GUI and via API."]}}